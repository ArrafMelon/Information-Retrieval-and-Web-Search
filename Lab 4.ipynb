{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8119b7-e2a7-4b8c-ae5f-580b4569fb65",
   "metadata": {},
   "source": [
    "# ðŸ§ª Lab Manual: Module 8 â€“ Web Crawling\n",
    "**Course:** Information Retrieval and Web Search  \n",
    "**Topics Covered:**\n",
    "- Web crawling strategies and architecture  \n",
    "- Politeness, robustness, and scalability  \n",
    "- URL normalization and deduplication  \n",
    "- Dynamic content handling and spam avoidance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0caa10-a4e1-4056-b055-16e949a27792",
   "metadata": {},
   "source": [
    "## <font color='red'>Submission: Submit both .ipynb file and .ipynb converted to PDF</font>\n",
    "## <font color='blue'>Submissions with following cases will get a zero</font>\n",
    "* ## <font color='red'>Code or commented text truncated from the pdf version of the notebook</font>\n",
    "* ### <font color='blue'>Any compilation error in the notebook</font>\n",
    "* ### <font color='blue'>Missing output for any of the programming cells. There should be an output for every code cell</font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746a4f3-740f-4613-a5f7-61a302f2c3ea",
   "metadata": {},
   "source": [
    "### Q1. Simulate Depth-First and Breadth-First Crawling  \n",
    "- Load a set of 4â€“6 web pages\n",
    "- Create a small directed graph representing a website (5â€“7 pages with links).  \n",
    "- Implement both DFS and BFS traversal.  \n",
    "- Print the order in which pages are visited.  \n",
    "- Comment on the differences in traversal behavior.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf7dc9b-1d01-401b-9bb7-c49d236242d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "## Following steps are optional to implement\n",
    "    #- Load a set of 4â€“6 web pages\n",
    "    #- Create a small directed graph representing a website (5â€“7 pages with links). \n",
    "# Sample website graph (Output of first 2 steps)\n",
    "# (You can implement first 2 steps if you want)\n",
    "web_graph = {\n",
    "    'Home': ['About', 'Products', 'Contact'],\n",
    "    'About': ['Team'],\n",
    "    'Products': ['Product1', 'Product2'],\n",
    "    'Contact': [],\n",
    "    'Team': [],\n",
    "    'Product1': [],\n",
    "    'Product2': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40b4dd6-80ca-4578-b541-4f9167354cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home', 'About', 'Team', 'Products', 'Product1', 'Product2', 'Contact']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DFS\n",
    "def DFS(web_graph, start_page):\n",
    "    visited = set()\n",
    "    stack = [start_page]\n",
    "    visited_order = []\n",
    "\n",
    "    while stack:\n",
    "        cur_page = stack.pop()\n",
    "\n",
    "        if cur_page not in visited:\n",
    "            visited.add(cur_page)\n",
    "            visited_order.append(cur_page)\n",
    "\n",
    "            # add linked pages to stack\n",
    "            for link in reversed(web_graph.get(cur_page, [])):\n",
    "                if link not in visited:\n",
    "                    stack.append(link)\n",
    "    \n",
    "    # Return the order of visited nodes\n",
    "    return visited_order\n",
    "\n",
    "# Test on given webpage\n",
    "DFS(web_graph, 'Home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01203025-84a8-4821-85df-9fdcfd906e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home', 'About', 'Products', 'Contact', 'Team', 'Product1', 'Product2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BFS\n",
    "def BFS(web_graph, start_page):\n",
    "    visited = set()\n",
    "    queue = deque([start_page])\n",
    "    visited_order = []\n",
    "\n",
    "    visited.add(start_page)\n",
    "\n",
    "    while queue:\n",
    "        # pop from front\n",
    "        cur_page = queue.popleft()\n",
    "        visited_order.append(cur_page)\n",
    "\n",
    "        # add linked pages to queue\n",
    "        for link in web_graph.get(cur_page, []):\n",
    "            if link not in visited:\n",
    "                queue.append(link)\n",
    "\n",
    "    # Return the order of visited nodes\n",
    "    return visited_order\n",
    "\n",
    "# Test on given webpage\n",
    "BFS(web_graph, 'Home')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe19e9-c4b9-4eb1-a699-863daa17d994",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "The traversal behaviour in BFS and DFS are very different. If we visualize our webpages to be a tree, DFS would start at the root node, and traverse as deep as it can into its first child node before moving onto the second child node, and this pattern continues for however big the tree is.\n",
    "\n",
    "However, in BFS this behaviour is different as we first go through every child node of the root before going further down level-by-level in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35eeb7-d8af-434d-b916-119397035322",
   "metadata": {},
   "source": [
    "### Q2. Focused Crawling with Priority Queue  \n",
    "- Assign a relevance score to each page (e.g., based on topic keywords).  \n",
    "- Use a priority queue to simulate focused crawling.  \n",
    "- Show how the crawler prioritizes topic-relevant pages.  \n",
    "- Visualize or print the crawl order.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62db5f41-44c2-4e0b-87e6-30c24756f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home', 'Products', 'Product1', 'Product2', 'Contact', 'About', 'Team']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code\n",
    "import heapq\n",
    "\n",
    "##Assign a relevance score to each page (e.g., based on topic keywords). \n",
    "# You can skip coding step 1 if you want and consider Sample pages with relevance scores\n",
    "pages = {\n",
    "    'Home': 0.2,\n",
    "    'About': 0.1,\n",
    "    'Products': 0.9,\n",
    "    'Product1': 0.95,\n",
    "    'Product2': 0.85,\n",
    "    'Contact': 0.3\n",
    "}\n",
    "\n",
    "def focused_crawling(web_graph, pages, start_page):\n",
    "    visited = set()\n",
    "\n",
    "    # -score becayse heapq=min heap\n",
    "    priority_queue = [(-pages.get(start_page, 0), start_page)]\n",
    "    visited_order = []\n",
    "\n",
    "    while priority_queue:\n",
    "        neg_score, cur_page = heapq.heappop(priority_queue)\n",
    "        score = -neg_score\n",
    "\n",
    "        if cur_page not in visited:\n",
    "            visited.add(cur_page)\n",
    "            visited_order.append(cur_page)\n",
    "\n",
    "            # Add link pages to priority queue\n",
    "            for link in web_graph.get(cur_page, []):\n",
    "                if link not in visited:\n",
    "                    link_score = pages.get(link,0)\n",
    "                    heapq.heappush(priority_queue, (-link_score, link))\n",
    "    return visited_order\n",
    "\n",
    "focused_crawling(web_graph, pages, 'Home')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bc342-49e2-41cb-9b5e-c371ab660bfd",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- The focused crawler assigns each page in the web graph a score based on relevance on how well it matches a specific topic. In our example, the product pages are more relevant than \"about\" or \"contact\". Using a max heap, we always visit the higest scored page next which is also unvisited. This means that relevant pages are crawled early, rather than just relying on the linked pages for crawling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127dd64-f48a-4397-83db-eceb211f3a43",
   "metadata": {},
   "source": [
    "### Q3. URL Normalization  \n",
    "- Write a function to normalize URLs (e.g., lowercase host, remove default ports, sort query params).  \n",
    "- Test it on a list of sample URLs. Add 4 more URLs to improve the list of urls. \n",
    "- Explain why normalization is critical for deduplication.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d246514-0661-4f94-93da-5a65385bd745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ExAmPlE.com/ -> http://example.com/\n",
      "http://example.com:80/ -> http://example.com/\n",
      "http://example.com/a/b/../c/ -> http://example.com/a/b/../c\n",
      "http://example.com/?b=2&a=1 -> http://example.com/?a=1&b=2\n",
      "http://example.com:123/path -> http://example.com:123/path\n",
      "http://Example.com/PATh?b=4&a=1 -> http://example.com/path?a=1&b=4\n",
      "http://ExAmPlE.com/a/b/ -> http://example.com/a/b\n",
      "https://ExAMPLE.com:443/path -> https://example.com/path\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode\n",
    "\n",
    "urls = [\n",
    "    \"http://ExAmPlE.com/\",\n",
    "    \"http://example.com:80/\",\n",
    "    \"http://example.com/a/b/../c/\",\n",
    "    \"http://example.com/?b=2&a=1\",\n",
    "    \"http://example.com:123/path\",\n",
    "    \"http://Example.com/PATh?b=4&a=1\",\n",
    "    \"http://ExAmPlE.com/a/b/\",\n",
    "    \"https://ExAMPLE.com:443/path\"\n",
    "]\n",
    "\n",
    "def url_norm(url):\n",
    "    # Parse URL\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # lowercase host\n",
    "    s = parsed_url.scheme.lower()\n",
    "\n",
    "    if parsed_url.hostname:\n",
    "        hn = parsed_url.hostname.lower()\n",
    "    else:\n",
    "        hn = \"\"\n",
    "\n",
    "    # remove default ports\n",
    "    port = parsed_url.port\n",
    "    if port and not((s==\"http\" and port==80) or (s==\"https\" and port==443)):\n",
    "        hn = f\"{hn}:{port}\"\n",
    "    \n",
    "    path = parsed_url.path.lower()\n",
    "    if path != '/' and path.endswith('/'):\n",
    "        path = path.rstrip('/')\n",
    "\n",
    "    #sort query params\n",
    "    query = ''\n",
    "    if parsed_url.query:\n",
    "        params = sorted(parse_qsl(parsed_url.query))\n",
    "        query = urlencode(params)\n",
    "\n",
    "    unparse_url = urlunparse((s, hn, path, '', query, ''))\n",
    "    return unparse_url\n",
    "\n",
    "# loop over urls and use function\n",
    "for url in urls:\n",
    "    print(f\"{url} -> {url_norm(url)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611426fb-4769-40f5-9d90-cf7a2cdf9dae",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- Normalization for URLs are important because we would like to reduce the number of redundant fetches of the same content. URls are normalized to a more simpler standard form, which makes it easier for duplication checking function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749c766-7b4f-46b4-b0c5-61373fef0dcc",
   "metadata": {},
   "source": [
    "\n",
    "### Q4. Duplicate Detection with Bloom Filter  \n",
    "- Simulate a Bloom Filter to detect duplicate URLs.  \n",
    "- Insert a set of normalized URLs and test for membership.  \n",
    "- Discuss false positives and memory efficiency.  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fb6791-1355-42ab-b9ac-8d807d22c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New: http://example.com/page1\n",
      "New: http://example.com/page2\n",
      "Duplicate: http://example.com/page1\n",
      "New: http://example.com/page3\n",
      "Duplicate: http://example.com/page3\n",
      "New: http://example.com/page4\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "urls = [\"http://example.com/page1\", \"http://example.com/page2\",\n",
    "        \"http://example.com/page1\", \"http://example.com/page3\",\n",
    "        \"http://example.com/page3\", \"http://example.com/page4\"]\n",
    "        # add more examples\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def bloom_filter(bit_arr, item, add=False):\n",
    "    size = len(bit_arr)\n",
    "    num_hash = 3\n",
    "\n",
    "    found = True\n",
    "    for i in range(num_hash):\n",
    "        hash_value = int(hashlib.md5(f\"{item}{i}\".encode()).hexdigest(),\n",
    "                         16)\n",
    "        idx = hash_value % size\n",
    "\n",
    "        if bit_arr[idx] == 0:\n",
    "            found=False\n",
    "            if add:\n",
    "                bit_arr[idx]=1\n",
    "        elif add:\n",
    "            bit_arr[idx]=1\n",
    "\n",
    "    return found\n",
    "\n",
    "bit_arr = [0]*100\n",
    "# find duplicates\n",
    "for url in urls:\n",
    "    if bloom_filter(bit_arr, url):\n",
    "        print(f\"Duplicate: {url}\")\n",
    "    else:\n",
    "        print(f\"New: {url}\")\n",
    "        bloom_filter(bit_arr, url, add=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d63f07-4fc7-4695-830e-3b14e6afb52e",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- Bloom filters can have false positives (i.e it can incorrectly report a url is a duplicate even though it hasnt been seen before), but not false negatives (detecting a already added url as new). Bloom filters are good with memory because it uses few bits per item (100 bits in our scenario) compared to storing entire url strings which would require way more memory (more than a couple of bytes), meaning bloom filters are probably at least 10x more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd98cf1-8041-4edb-aa9d-73eb0818d5d8",
   "metadata": {},
   "source": [
    "## Part 2: Crawler Features and Robustness\n",
    "---\n",
    "### Q5. robots.txt Compliance  \n",
    "- Write a parser that reads a sample robots.txt file.  \n",
    "- Determine whether a given URL is allowed for a specific User-Agent.  \n",
    "- Explain how this supports crawler politeness.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66072d8-9f67-4469-aedc-3deb179211b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "# You can use the following robots_txt (if you want)\n",
    "robots_txt = \"\"\"\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Global Directives (Applies to all compliant User-Agents)\n",
    "# -----------------------------------------------------------------\n",
    "User-agent: *\n",
    "Disallow: /cgi-bin/            # Block all common binary/script directories\n",
    "Disallow: /admin/              # Block primary administrative backend (MUST be secured otherwise!)\n",
    "Disallow: /private/            # Block internal, private content\n",
    "Disallow: /search              # Block internal search results pages (prevents duplicate content)\n",
    "Disallow: /*?                   # Block URLs containing a query string (e.g., /page?id=123)\n",
    "Disallow: /wp-includes/        # (If using WordPress) Block core internal files\n",
    "Disallow: /feed/               # Block all common site feeds (often crawled unnecessarily)\n",
    "\n",
    "# Disallow specific files that are large, sensitive, or contain duplicate content\n",
    "Disallow: /old-sitemap.xml\n",
    "Disallow: /backup-data-2025.zip\n",
    "Disallow: /testing-page.html\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Specific Directives for Google (Googlebot)\n",
    "# -----------------------------------------------------------------\n",
    "User-agent: Googlebot\n",
    "Disallow: /api/v1/             # Block only Google from crawling specific API endpoints\n",
    "Disallow: /temp/images/        # Block a temporary image folder only for Google\n",
    "\n",
    "# Override a global block: allow Google to crawl a specific path within a disallowed directory (Optional)\n",
    "# This is often unnecessary if the global rule is defined correctly, but is useful for exceptions.\n",
    "# Allow: /private/public-doc.pdf\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Specific Directives for Bing (Bingbot)\n",
    "# -----------------------------------------------------------------\n",
    "User-agent: Bingbot\n",
    "Crawl-delay: 5                 # Request Bingbot wait 5 seconds between requests (helpful for server load)\n",
    "Disallow: /staging/            # Block Bing from crawling staging environments\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Sitemap Location\n",
    "# -----------------------------------------------------------------\n",
    "# Always include the sitemap at the end of the file so crawlers can easily find it.\n",
    "Sitemap: https://www.yourdomain.com/sitemap.xml\n",
    "Sitemap: https://www.yourdomain.com/video-sitemap.xml\n",
    "\"\"\"\n",
    "\n",
    "def compliance(robots, url, user_agent):\n",
    "    lines = robots.strip().split('\\n')\n",
    "    cur_agent=None\n",
    "    disallow=[]\n",
    "    allow=[]\n",
    "    found_agent = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        if ':' in line:\n",
    "            key,value = line.split(':',1)\n",
    "            key = key.strip().lower()\n",
    "            value=value.strip()\n",
    "\n",
    "            if '#' in value:\n",
    "                value = value.split('#')[0].strip()\n",
    "\n",
    "            if key=='user-agent':\n",
    "                cur_agent=value.lower()\n",
    "\n",
    "                if cur_agent == user_agent.lower():\n",
    "                    found_agent=True\n",
    "                    disallow=[]\n",
    "                    allow=[]\n",
    "                elif not found_agent and cur_agent==\"*\":\n",
    "                    disallow=[]\n",
    "                    allow=[]\n",
    "            elif cur_agent==user_agent.lower():\n",
    "                if key==\"disallow\":\n",
    "                    disallow.append(value)\n",
    "                elif key==\"allow\":\n",
    "                    allow.append(value)\n",
    "\n",
    "            elif not found_agent and cur_agent==\"*\":\n",
    "                if key==\"disallow\":\n",
    "                    disallow.append(value)\n",
    "                elif key==\"allow\":\n",
    "                    allow.append(value)\n",
    "            \n",
    "        # check allow\n",
    "    for path in allow:\n",
    "        if url.startswith(path):\n",
    "            return True\n",
    "\n",
    "        # disallow\n",
    "    for path in disallow:\n",
    "        if url.startswith(path):\n",
    "            return False\n",
    "   \n",
    "    return True\n",
    "\n",
    "# Tests\n",
    "print(compliance(robots_txt,'/index.html','*'))\n",
    "print(compliance(robots_txt,'/admin/','*'))\n",
    "print(compliance(robots_txt,'/api/v1/ ','Googlebot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1b2e4-afa5-4725-ad10-591a9beb10ca",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- This function using robots.txt supports politeness by preventing overload and respecting site owner wishes. It avoids restricted pages and follows the crawl delay. This allows for efficient use of bandwidth by avoiding indexing content the owners dont want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab86dc-5e82-437d-b974-82f91d6d87d8",
   "metadata": {},
   "source": [
    "### Q6. Handling Malformed HTML  \n",
    "- Load a malformed HTML snippet.  \n",
    "- Use BeautifulSoup to extract text and links.  \n",
    "- Demonstrate resilience to broken tags and non-standard markup.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069967f8-429a-4fcf-ba76-85b7f0a22b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Missing closing tag Another unclosed paragraph Link 1 Link 2 No quotes Nested without closing',\n",
       " [{'url': 'http://example.com', 'text': 'Link 1'},\n",
       "  {'url': 'http://test.com', 'text': 'Link 2'},\n",
       "  {'url': 'http://broken.com', 'text': 'No quotes'}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code\n",
    "# You can prepare your html page if you want, print the html page content in the notebook to show issues\n",
    "from bs4 import BeautifulSoup as bs\n",
    "def malformed_html(html_tag):\n",
    "    soup = bs(html_tag, 'html.parser')\n",
    "\n",
    "    text = soup.get_text(separator=' ',strip = True)\n",
    "\n",
    "    #extract links\n",
    "    links=[]\n",
    "    for a_tag in soup.find_all('a',href=True):\n",
    "        links.append({\n",
    "            'url':a_tag['href'],\n",
    "            'text':a_tag.get_text(strip=True)\n",
    "        })\n",
    "    return text, links\n",
    "\n",
    "malformed_html_tag = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Missing closing tag\n",
    "    <p>Another unclosed paragraph\n",
    "    <a href=\"http://example.com\">Link 1</a>\n",
    "    <a href='http://test.com'>Link 2</a>\n",
    "    <a href=http://broken.com>No quotes</a>\n",
    "    <div><p>Nested without closing\n",
    "</body>\n",
    "\"\"\"\n",
    "\n",
    "malformed_html(malformed_html_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82387a9-e3a5-459b-8412-cda525e09032",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- This parser function for an HTML tag is very efficient in extracting content from a messy html webpage. It is good enough when there are missing elemts, broken tags and odd markups in the html webpage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec30ad-4b20-41dc-b556-16fa4655e1af",
   "metadata": {},
   "source": [
    "## Part 3: DNS, Concurrency, and Architecture\n",
    "---\n",
    "### Q7. DNS Resolution and Caching  \n",
    "- Simulate DNS lookups with and without caching.  \n",
    "- Measure lookup time and show cache hits/misses.  \n",
    "- Discuss TTL and its impact on freshness.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a94d14-30a5-4fdc-b25a-f196ce5f7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Cache:\n",
      "Cache miss: google.com\n",
      "google.com -> 192.178.192.100 (18 ms)\n",
      "Cache miss: github.com\n",
      "github.com -> 140.82.112.3 (11 ms)\n",
      "Cache Hit: google.com\n",
      "google.com -> 192.178.192.100 (0 ms)\n",
      "Cache Hit: github.com\n",
      "github.com -> 140.82.112.3 (0 ms)\n",
      "\n",
      "\n",
      "Without Cache:\n",
      "Cache miss: google.com\n",
      "google.com -> 192.178.192.100 (1 ms)\n",
      "Cache miss: github.com\n",
      "github.com -> 140.82.112.3 (0 ms)\n",
      "Cache miss: google.com\n",
      "google.com -> 192.178.192.100 (1 ms)\n",
      "Cache miss: github.com\n",
      "github.com -> 140.82.112.3 (0 ms)\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "import socket\n",
    "import time\n",
    "\n",
    "dns_cache = {}\n",
    "\n",
    "def dns_lookup(hostname, use_cache=True, ttl=300):\n",
    "    cur_time = time.time()\n",
    "\n",
    "    if use_cache and hostname in dns_cache:\n",
    "        cached_ip, timestamp=dns_cache[hostname]\n",
    "        if cur_time-timestamp<ttl:\n",
    "            print(f\"Cache Hit: {hostname}\")\n",
    "            return cached_ip, 0.0\n",
    "\n",
    "    print(f\"Cache miss: {hostname}\")\n",
    "    time_start = time.time()\n",
    "    try:\n",
    "        ip = socket.gethostbyname(hostname)\n",
    "        elapsed = time.time()-time_start\n",
    "        if use_cache:\n",
    "            dns_cache[hostname]=(ip,cur_time)\n",
    "        return ip,elapsed\n",
    "    except:\n",
    "        return None, time.time()-time_start\n",
    "\n",
    "hostnames = ['google.com', 'github.com','google.com','github.com']\n",
    "\n",
    "print(\"With Cache:\")\n",
    "for name in hostnames:\n",
    "    ip,t = dns_lookup(name, use_cache=True)\n",
    "    print(f\"{name} -> {ip} ({t*1000:.0f} ms)\")\n",
    "print('\\n')\n",
    "print(\"Without Cache:\")\n",
    "for name in hostnames:\n",
    "    ip,t = dns_lookup(name, use_cache=False)\n",
    "    print(f\"{name} -> {ip} ({t*1000:.0f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf994b-7fde-4853-bd38-4ce904ed041c",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- a smaller TTL means there are more lookups but fresher data. A higher TTL means that there are less lookups but the data is not accurate if the IP changes more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4d0e5-f3f6-45aa-b188-3eefd99a69db",
   "metadata": {},
   "source": [
    "### Q8. Asynchronous Fetching  \n",
    "- Use asyncio or threading to simulate concurrent page fetching.  \n",
    "- Show how concurrency improves throughput.  \n",
    "- Compare with sequential fetching.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dcb6677-6e8b-4bf4-ba29-27c2dc041cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent\n",
      "Time: 1.15s\n",
      "\n",
      "\n",
      "Time: 5.85s\n"
     ]
    }
   ],
   "source": [
    "#Code\n",
    "import time \n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fetch(url):\n",
    "    response = requests.get(url)\n",
    "    return response.status_code\n",
    "\n",
    "urls = [\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1'\n",
    "]\n",
    "\n",
    "print(\"Concurrent\")\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.map(fetch, urls)\n",
    "con_time = time.time()-start\n",
    "print(f\"Time: {con_time:.2f}s\")\n",
    "\n",
    "print('\\n')\n",
    "start = time.time()\n",
    "for url in urls:\n",
    "    fetch(url)\n",
    "seq_time = time.time()-start\n",
    "print(f\"Time: {seq_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab1f3-cd1c-4b1f-ab75-8bc7c75d32fa",
   "metadata": {},
   "source": [
    ">Theoretical part Answer here\n",
    "- Sequential fetching proccesses 1 url at a time hence why it took 5+ seconds in total (one by one, and the time is added up). Concurrent fetching fetches all urls simeoultaneously using threading. It complestes in one second because it fetches in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f54c2-aeca-493f-99e3-be4ec32736a4",
   "metadata": {},
   "source": [
    "### Q9. Modular Crawler Architecture  \n",
    "- Describe the modular architecture (Scheduler, Fetcher, Parser, Storage).  \n",
    "- Explain the role of each module and how they interact.  \n",
    "- Discuss how this design supports scalability and fault tolerance.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66487c31-c4d6-4597-96ea-df36fd8ee571",
   "metadata": {},
   "source": [
    ">Answer here (Theoretical question)\n",
    "- The scheduler is the main unit of the architecture. It is the decision making tool which decides which page to crawl next using the alogirthms it is provided.\n",
    "- The fetcher is the tool of the architecture. it does not make the decisions, but does the heavy lifting by downloading the content of a provided URL. It uses the schedulers decision URL to fetch.\n",
    "- The parser uses the fetchers output to read the content, extracting texts and other media, as well as recognizing other links in the document. The links from the parser gets fed back into the scheduler\n",
    "- The storage gets the text from the parser and stores the crawled data into a database. It stores the index of the page, the connected links of different pages, and any other important meta data\n",
    "\n",
    "Horizontal scaling using multiple machines working in parallel can be used to improve the scalability and efficiency of the architecture. This goes hand in hand with fault tolerance. If one machine fails, another machine with the same data is running, no data at any moment is lost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
